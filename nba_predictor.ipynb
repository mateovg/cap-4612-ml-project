{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df_games = pd.read_csv('data/games_rolling.csv')\n",
    "    df_games = df_games.select_dtypes(include=['float64', 'int64'])\n",
    "    # df_games.drop(['season_id', 'team_id_home', 'game_id', 'team_id_away'], axis=1, inplace=True)\n",
    "    # df_games.drop(['team_id_home', 'game_id', 'team_id_away'], axis=1, inplace=True)\n",
    "    return df_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy():\n",
    "    data = get_data()\n",
    "\n",
    "    ignored_cols = ['season_id', 'team_id_home',\n",
    "                    'game_id', 'team_id_away', 'wl_home']\n",
    "    feature_cols = data.columns[~data.columns.isin(ignored_cols)]\n",
    "    target_col = 'wl_home'\n",
    "\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metrics(true, pred):\n",
    "    # find the metrics for the binary classification model\n",
    "\n",
    "    # some bitwise operations to find the metrics\n",
    "    true_positives = ((true == 1) & (pred == 1)).sum()\n",
    "    false_positives = ((true == 0) & (pred == 1)).sum()\n",
    "    true_negatives = ((true == 0) & (pred == 0)).sum()\n",
    "    false_negatives = ((true == 1) & (pred == 0)).sum()\n",
    "    total = true_positives + false_positives + true_negatives + false_negatives\n",
    "\n",
    "    \n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / total\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "def print_metrics(labels, pred):\n",
    "    metrics = find_metrics(labels, pred)\n",
    "    print(f\"Accuracy: {metrics['accuracy']:0.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:0.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:0.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix graphic.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): The true target values.\n",
    "        y_pred (array-like): The predicted target values.\n",
    "        classes (list): List of class labels (strings) in the order of the confusion matrix.\n",
    "        title (str): Title of the confusion matrix plot.\n",
    "        cmap (matplotlib colormap): Colormap to be used for the heatmap (default is plt.cm.Blues).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_normalized, annot=True, cmap=cmap, fmt=\".2f\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# time series split for cross validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# classifiers we will use\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n",
      "Precision: nan\n",
      "Recall: nan\n",
      "F1 Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvela\\AppData\\Local\\Temp\\ipykernel_45420\\1131102829.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  accuracy = (true_positives + true_negatives) / total\n",
      "C:\\Users\\mvela\\AppData\\Local\\Temp\\ipykernel_45420\\1131102829.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = true_positives / (true_positives + false_positives)\n",
      "C:\\Users\\mvela\\AppData\\Local\\Temp\\ipykernel_45420\\1131102829.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall = true_positives / (true_positives + false_negatives)\n"
     ]
    }
   ],
   "source": [
    "# elo base model\n",
    "X, y = get_xy()\n",
    "elo_preds = [1 if elo_home + 100> elo_away else 0 for elo_home, elo_away in zip(X['elo_home'], X['elo_away']) ]\n",
    "print_metrics(y, elo_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_test(data, model, features):\n",
    "    # test the model on previous seasons iteratively\n",
    "    # each iteration, the model is trained on all previous seasons\n",
    "    # and tested on the current season\n",
    "    target = 'wl_home'\n",
    "    seasons = data['season_id'].unique()\n",
    "\n",
    "    all_predictions = pd.DataFrame(columns=['actual', 'predicted'])\n",
    "\n",
    "    for i in range(2, len(seasons)):\n",
    "        # start on the 3rd season\n",
    "        season = seasons[i]\n",
    "        train = data[data['season_id'] < season]\n",
    "        test = data[data['season_id'] == season]\n",
    "\n",
    "        X_train, y_train = train[features], train[target]\n",
    "        X_test, y_test = test[features], test[target]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        season_predictions = pd.DataFrame(\n",
    "            {'actual': y_test, 'predicted': predictions}\n",
    "        )\n",
    "        all_predictions = pd.concat([all_predictions, season_predictions])\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def find_best_features(data, model, n_features):\n",
    "    # find the best features using sequential feature selector\n",
    "    ignored_cols = ['season_id', 'team_id_home',\n",
    "                    'game_id', 'team_id_away', 'wl_home']\n",
    "    feature_cols = data.columns[~data.columns.isin(ignored_cols)]\n",
    "    target_col = 'wl_home'\n",
    "\n",
    "    # 5 fold cross validation for time series data\n",
    "    split = TimeSeriesSplit(n_splits=5)\n",
    "    # finds the best features using sequential feature selector\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        model, n_features_to_select=n_features, cv=split, n_jobs=-1)\n",
    "\n",
    "    # Create a copy of the data to avoid modifying the original data\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Normalize the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    data_copy[feature_cols] = scaler.fit_transform(data_copy[feature_cols])\n",
    "\n",
    "    # fit the model\n",
    "    sfs.fit(data_copy[feature_cols], data_copy[target_col])\n",
    "\n",
    "    return list(feature_cols[sfs.get_support()])\n",
    "\n",
    "\n",
    "def evaluate(data, model, n_features):\n",
    "    print(f'Finding best {n_features} features...')\n",
    "    best_features = find_best_features(data, model, n_features)\n",
    "    print(f'Best features: {best_features}')\n",
    "\n",
    "    print('Back testing model...')\n",
    "    predictions = back_test(data, model, best_features)\n",
    "\n",
    "    print('Metrics:')\n",
    "    actual, predicted = predictions['actual'], predictions['predicted']\n",
    "    print_metrics(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6752\n",
      "Precision: 0.6869\n",
      "Recall: 0.8426\n",
      "F1 Score: 0.7568\n"
     ]
    }
   ],
   "source": [
    "data = get_data()\n",
    "predictions = back_test(data, LogisticRegression(), ['elo_home', 'elo_away', 'plus_minus_home', 'plus_minus_away']).reset_index(drop=True)\n",
    "\n",
    "# get metrics from predictions\n",
    "actual, predicted = predictions['actual'], predictions['predicted']\n",
    "\n",
    "print_metrics(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6545\n",
      "Precision: 0.7563\n",
      "Recall: 0.6286\n",
      "F1 Score: 0.6865\n",
      "Accuracy: 0.6019\n",
      "Precision: 0.6019\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.7515\n"
     ]
    }
   ],
   "source": [
    "data = get_data()\n",
    "true = data['wl_home']\n",
    "pred = data['elo_home'] > data['elo_away']\n",
    "print_metrics(true, pred)\n",
    "\n",
    "# baseline, always predict home team wins\n",
    "pred = 1\n",
    "print_metrics(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best 5 features...\n",
      "Best features: ['fg3a_home', 'fg3_pct_home', 'elo_home', 'fg_pct_away', 'elo_away']\n",
      "Back testing model...\n",
      "Metrics:\n",
      "Accuracy: 0.6317\n",
      "Precision: 0.6753\n",
      "Recall: 0.7435\n",
      "F1 Score: 0.7077\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "evaluate(data, knn, 5) \n",
    "evaluate(data, knn, 3)\n",
    "evaluate(data, knn, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

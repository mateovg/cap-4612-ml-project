{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_target():\n",
    "    df_games = pd.read_csv('data/games_rolling.csv')\n",
    "    df_games = df_games.select_dtypes(include=['float64', 'int64'])\n",
    "    df_games.drop(['season_id', 'team_id_home', 'game_id', 'team_id_away'], axis=1, inplace=True)\n",
    "\n",
    "    features = df_games.drop(columns=['wl_home', 'wl_away'])\n",
    "    target = df_games['wl_home']\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will separate the features from the target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41865 games with 40 features\n",
      "Index(['fgm_home', 'fga_home', 'fg_pct_home', 'fg3m_home', 'fg3a_home',\n",
      "       'fg3_pct_home', 'ftm_home', 'fta_home', 'ft_pct_home', 'oreb_home',\n",
      "       'dreb_home', 'reb_home', 'ast_home', 'stl_home', 'blk_home', 'tov_home',\n",
      "       'pf_home', 'pts_home', 'plus_minus_home', 'elo_home', 'fgm_away',\n",
      "       'fga_away', 'fg_pct_away', 'fg3m_away', 'fg3a_away', 'fg3_pct_away',\n",
      "       'ftm_away', 'fta_away', 'ft_pct_away', 'oreb_away', 'dreb_away',\n",
      "       'reb_away', 'ast_away', 'stl_away', 'blk_away', 'tov_away', 'pf_away',\n",
      "       'pts_away', 'plus_minus_away', 'elo_away'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features, target = get_features_target()\n",
    "\n",
    "print(f\"{features.shape[0]} games with {features.shape[1]} features\")\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy of the model will be predicting the home team always wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model\n",
      "Accuracy: 0.6024\n",
      "Precision: 0.6024\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.7519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def find_metrics(labels, preds):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds),\n",
    "        'recall': recall_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "def print_metrics(labels, preds):\n",
    "    metrics = find_metrics(labels, preds)\n",
    "    print(f\"Accuracy: {metrics['accuracy']:0.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:0.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:0.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:0.4f}\")\n",
    "\n",
    "# baseline model\n",
    "baseline_preds = [1] * len(target) # home team always wins\n",
    "print(\"Baseline Model\")\n",
    "print_metrics(target, baseline_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fgm_home', 'fga_home', 'fg_pct_home', 'fg3m_home', 'fg3a_home',\n",
       "       'fg3_pct_home', 'ftm_home', 'fta_home', 'ft_pct_home', 'oreb_home',\n",
       "       'dreb_home', 'reb_home', 'ast_home', 'stl_home', 'blk_home', 'tov_home',\n",
       "       'pf_home', 'pts_home', 'plus_minus_home', 'elo_home', 'fgm_away',\n",
       "       'fga_away', 'fg_pct_away', 'fg3m_away', 'fg3a_away', 'fg3_pct_away',\n",
       "       'ftm_away', 'fta_away', 'ft_pct_away', 'oreb_away', 'dreb_away',\n",
       "       'reb_away', 'ast_away', 'stl_away', 'blk_away', 'tov_away', 'pf_away',\n",
       "       'pts_away', 'plus_minus_away', 'elo_away'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `StandardScalar` from `sklearn.preprocessing`, which standardizes data with it's z-score for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series split for cross validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def train(model, cv = tscv, n_splits = 5, X = features, y = target, scaler=scaler):\n",
    "    X_scaled = scaler.fit_transform(X) if scaler else X\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for train_index, test_index in cv.split(features_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "\n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6139\n",
      "Precision: 0.6673\n",
      "Recall: 0.6331\n",
      "F1 Score: 0.6498\n"
     ]
    }
   ],
   "source": [
    "# find best features using SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "X, y = get_features_target()\n",
    "test, pred = train(knn, X=X, y=y)\n",
    "print_metrics(test, pred)\n",
    "\n",
    "# sfs = SequentialFeatureSelector(knn, n_features_to_select=30, cv=cv, n_jobs=-1)\n",
    "# sfs.fit(X, y)\n",
    "\n",
    "# get the best features\n",
    "# best_features = X.columns[sfs.get_support()]\n",
    "# X = X[best_features]\n",
    "\n",
    "# export best features since it took 36 minutes to find them\n",
    "# X.to_csv('data/best_features_knn.csv', index=False)\n",
    "\n",
    "best_features_knn = pd.read_csv('data/best_features_knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 13\n",
      "Accuracy: 0.6206\n",
      "Precision: 0.6451\n",
      "Recall: 0.7322\n",
      "F1 Score: 0.6859\n",
      "Accuracy: 0.6139\n",
      "Precision: 0.6673\n",
      "Recall: 0.6331\n",
      "F1 Score: 0.6498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def find_best_k(X, y, max_k=15):\n",
    "    best_n = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        test, pred = train(knn, X=X, y=y)\n",
    "        score = accuracy_score(test, pred)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n = k\n",
    "\n",
    "    return (best_n, best_score)\n",
    "\n",
    "best_n, best_score = find_best_k(X, y)\n",
    "\n",
    "def print_best_knn(X, y, max_k=15, best_n = 0):\n",
    "    if best_n != 0:\n",
    "        print(f\"Best n_neighbors: {best_n}\")\n",
    "        test, pred = train(KNeighborsClassifier(n_neighbors=best_n))\n",
    "        print_metrics(test, pred)\n",
    "        return\n",
    "    best_n, best_score = find_best_k(X, y, max_k)\n",
    "    test, pred = train(KNeighborsClassifier(n_neighbors=best_n))\n",
    "    print(f\"Best n_neighbors: {best_n}\")\n",
    "    print_metrics(test, pred)\n",
    "\n",
    "print_best_knn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6252\n",
      "Precision: 0.6609\n",
      "Recall: 0.6929\n",
      "F1 Score: 0.6766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# find best k features\n",
    "def find_best_k_features(X, y, max_k=30):\n",
    "    best_k = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        X_new = SelectKBest(f_classif, k=k).fit_transform(X, y)\n",
    "        knn = KNeighborsClassifier(n_neighbors=best_n)\n",
    "        test, pred = train(knn, X=X_new)\n",
    "        score = accuracy_score(test, pred)\n",
    "        if score > best_score:\n",
    "            best_features = X_new\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    return (best_k, best_score, best_features, test, pred)\n",
    "\n",
    "features = pd.read_csv('data/best_features.csv')\n",
    "best_k, best_score, best_features, test, pred = find_best_k_features(X, y)\n",
    "print_metrics(test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65.11% accuracy is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6369\n",
      "Precision: 0.6884\n",
      "Recall: 0.6544\n",
      "F1 Score: 0.6710\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "rfc = RandomForestClassifier()\n",
    "X, y = get_features_target()\n",
    "\n",
    "# sfs = SequentialFeatureSelector(rfc, n_features_to_select=5, cv=cv, n_jobs=-1)\n",
    "# sfs.fit(X, y)\n",
    "\n",
    "# # get the best features\n",
    "# best_features = X.columns[sfs.get_support()]\n",
    "\n",
    "\n",
    "# # export best features since it took 36 minutes to find them\n",
    "# X.to_csv('data/best_features_rfc.csv', index=False)\n",
    "\n",
    "# X = pd.read_csv('data/best_features_rfc.csv')\n",
    "# best_features = X.columns\n",
    "test, pred = train(RandomForestClassifier(), X=X, y=y)\n",
    "print_metrics(test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression and the SVM with a linear kernel both performed better than the baseline. We will try to improve both models using their hyperparameters. From https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "['fga_home', 'fg_pct_home', 'fg3_pct_home', 'ftm_home', 'fta_home', 'ft_pct_home', 'oreb_home', 'reb_home', 'ast_home', 'stl_home', 'blk_home', 'tov_home', 'pf_home', 'plus_minus_home', 'elo_home', 'fg_pct_away', 'fg3m_away', 'fg3_pct_away', 'ftm_away', 'fta_away', 'ft_pct_away', 'dreb_away', 'reb_away', 'ast_away', 'stl_away', 'blk_away', 'tov_away', 'pf_away', 'plus_minus_away', 'elo_away']\n"
     ]
    }
   ],
   "source": [
    "# using grid search to find best parameters\n",
    "\n",
    "# X = pd.read_csv('data/best_features_rfc.csv')\n",
    "# y = pd.read_csv('data/games.csv')['wl_home']\n",
    "\n",
    "def find_best_params(X, y, model, params):\n",
    "    grid = GridSearchCV(model, params, cv=TimeSeriesSplit(), scoring=accuracy_score ,n_jobs=-1, verbose=2)\n",
    "    return grid.fit(X, y)\n",
    "\n",
    "# params = {\n",
    "#     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     'max_depth': [None, 5, 10, 15, 20],\n",
    "#     'min_samples_split': [2, 5, 10, 15, 20],\n",
    "#     'min_samples_leaf': [1, 2, 5, 10, 15, 20]\n",
    "# }\n",
    "\n",
    "# best_params = find_best_params(X, y, RandomForestClassifier(), params)\n",
    "\n",
    "# took 90 minutes to find best parameters\n",
    "# pd.DataFrame(best_params, index=[0]).to_csv('data/best_params_rfc.csv', index=False)\n",
    "best_params = pd.read_csv('data/best_params_rfc.csv').to_dict('records')[0]\n",
    "\n",
    "print(best_params)\n",
    "print(best_features.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6709\n",
      "Precision: 0.6946\n",
      "Recall: 0.7464\n",
      "F1 Score: 0.7196\n"
     ]
    }
   ],
   "source": [
    "y_test, pred = train(RandomForestClassifier(**best_params), X=X, y=y)\n",
    "print_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6734\n",
      "Precision: 0.6982\n",
      "Recall: 0.7444\n",
      "F1 Score: 0.7205\n",
      "['ftm_home', 'plus_minus_home', 'elo_home', 'oreb_away', 'elo_away']\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "logreg = LogisticRegression(max_iter = 1000, n_jobs=-1, verbose=2)\n",
    "X, y = get_features_target()\n",
    "test, pred = train(logreg, X=X, y=y, scaler=MinMaxScaler())\n",
    "print_metrics(test, pred)\n",
    "\n",
    "# find best features using SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(logreg, n_features_to_select=5, cv=cv, n_jobs=-1)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# get the best features\n",
    "best_features = X.columns[sfs.get_support()]\n",
    "X = X[best_features]\n",
    "print(best_features.to_list())\n",
    "\n",
    "X.to_csv('data/best_features_logreg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6771\n",
      "Precision: 0.6942\n",
      "Recall: 0.7672\n",
      "F1 Score: 0.7288\n"
     ]
    }
   ],
   "source": [
    "test, preds = train(logreg, X=X, y=y, scaler=MinMaxScaler())\n",
    "print_metrics(test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6763\n",
      "Precision: 0.6881\n",
      "Recall: 0.8464\n",
      "F1 Score: 0.7591\n"
     ]
    }
   ],
   "source": [
    "# elo base model\n",
    "elo_home = X['elo_home']\n",
    "elo_preds = [1 if elo_home + 100> elo_away else 0 for elo_home, elo_away in zip(X['elo_home'], X['elo_away']) ]\n",
    "print_metrics(y, elo_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

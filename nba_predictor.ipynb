{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_target():\n",
    "    df_games = pd.read_csv('data/games_rolling.csv')\n",
    "    df_games = df_games.select_dtypes(include=['float64', 'int64'])\n",
    "    df_games.drop(['season_id', 'team_id_home', 'game_id', 'team_id_away'], axis=1, inplace=True)\n",
    "\n",
    "    features = df_games.drop(columns=['wl_home', 'wl_away'])\n",
    "    target = df_games['wl_home']\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will separate the features from the target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41865 games with 40 features\n",
      "Index(['fgm_home', 'fga_home', 'fg_pct_home', 'fg3m_home', 'fg3a_home',\n",
      "       'fg3_pct_home', 'ftm_home', 'fta_home', 'ft_pct_home', 'oreb_home',\n",
      "       'dreb_home', 'reb_home', 'ast_home', 'stl_home', 'blk_home', 'tov_home',\n",
      "       'pf_home', 'pts_home', 'plus_minus_home', 'elo_home', 'fgm_away',\n",
      "       'fga_away', 'fg_pct_away', 'fg3m_away', 'fg3a_away', 'fg3_pct_away',\n",
      "       'ftm_away', 'fta_away', 'ft_pct_away', 'oreb_away', 'dreb_away',\n",
      "       'reb_away', 'ast_away', 'stl_away', 'blk_away', 'tov_away', 'pf_away',\n",
      "       'pts_away', 'plus_minus_away', 'elo_away'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features, target = get_features_target()\n",
    "\n",
    "print(f\"{features.shape[0]} games with {features.shape[1]} features\")\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy of the model will be predicting the home team always wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6024\n",
      "Precision: 0.6024\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.7519\n",
      "Accuracy: 0.6763\n",
      "Precision: 0.6881\n",
      "Recall: 0.8463\n",
      "F1 Score: 0.7590\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def find_metrics(labels, preds):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds),\n",
    "        'recall': recall_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "def print_metrics(labels, preds):\n",
    "    metrics = find_metrics(labels, preds)\n",
    "    print(f\"Accuracy: {metrics['accuracy']:0.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:0.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:0.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:0.4f}\")\n",
    "\n",
    "# baseline model\n",
    "baseline_preds = [1] * len(target) # home team always wins\n",
    "print(\"Baseline Model\")\n",
    "print_metrics(target, baseline_preds)\n",
    "\n",
    "# elo predictions, elo_home + 100 > elo_away\n",
    "elo_preds = [1 if x + 100 > y else 0 for (x,y) in zip(features['elo_home'], features['elo_away'])]\n",
    "print(\"Elo Model\")\n",
    "print_metrics(target, elo_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fgm_home', 'fga_home', 'fg_pct_home', 'fg3m_home', 'fg3a_home',\n",
       "       'fg3_pct_home', 'ftm_home', 'fta_home', 'ft_pct_home', 'oreb_home',\n",
       "       'dreb_home', 'reb_home', 'ast_home', 'stl_home', 'blk_home', 'tov_home',\n",
       "       'pf_home', 'pts_home', 'plus_minus_home', 'elo_home', 'fgm_away',\n",
       "       'fga_away', 'fg_pct_away', 'fg3m_away', 'fg3a_away', 'fg3_pct_away',\n",
       "       'ftm_away', 'fta_away', 'ft_pct_away', 'oreb_away', 'dreb_away',\n",
       "       'reb_away', 'ast_away', 'stl_away', 'blk_away', 'tov_away', 'pf_away',\n",
       "       'pts_away', 'plus_minus_away', 'elo_away'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `StandardScalar` from `sklearn.preprocessing`, which standardizes data with it's z-score for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series split for cross validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def train(model, cv = tscv, n_splits = 5, X = features, y = target, scaler=scaler):\n",
    "    X_scaled = scaler.fit_transform(X) if scaler else X\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for train_index, test_index in cv.split(features_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "    return y_test, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best features using SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "X, y = get_features_target()\n",
    "\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select=30, cv=cv, n_jobs=-1)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# get the best features\n",
    "best_features = X.columns[sfs.get_support()]\n",
    "X = X[best_features]\n",
    "\n",
    "# export best features since it took 36 minutes to find them\n",
    "X.to_csv('data/best_features_knn.csv', index=False)\n",
    "\n",
    "best_featres_knn = pd.read_csv('data/best_features_knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 11\n",
      "Best score: 0.6306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def find_best_k(X, y, max_k=15):\n",
    "    best_n = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        test, preds = train(knn, X=X, y=y)\n",
    "        score = accuracy_score(test, preds)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n = k\n",
    "\n",
    "    return (best_n, best_score)\n",
    "\n",
    "best_n, best_score = find_best_k(X, y)\n",
    "\n",
    "def print_best_knn(X, y, max_k=15, best_n = 0):\n",
    "    if best_n != 0:\n",
    "        print(f\"Best n_neighbors: {best_n}\")\n",
    "        test, pred = train(KNeighborsClassifier(n_neighbors=best_n))\n",
    "        print_metrics(test, pred)\n",
    "        return\n",
    "    best_n, best_score = find_best_k(X, y, max_k)\n",
    "    print(f\"Best n_neighbors: {best_n}\")\n",
    "    print_metrics(test, pred)\n",
    "\n",
    "print_best_knn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 4\n",
      "Best features: 4\n",
      "Accuracy: 0.6318\n",
      "Precision: 0.6540\n",
      "Recall: 0.7413\n",
      "F1 Score: 0.6949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# find best k features\n",
    "\n",
    "def find_best_k_features(X, y, max_k=30):\n",
    "    best_k = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        X_new = SelectKBest(f_classif, k=k).fit_transform(X, y)\n",
    "        knn = KNeighborsClassifier(n_neighbors=best_n)\n",
    "        test, preds = train(knn, X=X_new)\n",
    "        score = accuracy_score(test, preds)\n",
    "        if score > best_score:\n",
    "            best_features = X_new\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    return (best_k, best_score, best_features, test, preds)\n",
    "\n",
    "features = pd.read_csv('data/best_features.csv')\n",
    "best_k, best_score, best_features, test, preds = find_best_k_features(X, y)\n",
    "print_metrics(test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65.11% accuracy is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6572\n",
      "Precision: 0.6896\n",
      "Recall: 0.7165\n",
      "F1 Score: 0.7028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# cv = TimeSeriesSplit(n_splits=5)\n",
    "# rfc = RandomForestClassifier()\n",
    "# X, y = get_features_target()\n",
    "\n",
    "# sfs = SequentialFeatureSelector(rfc, n_features_to_select=30, cv=cv, n_jobs=-1, verbose=2)\n",
    "# sfs.fit(X, y)\n",
    "\n",
    "# # get the best features\n",
    "# best_features = X.columns[sfs.get_support()]\n",
    "# X = X[best_features]\n",
    "\n",
    "# # export best features since it took 36 minutes to find them\n",
    "# X.to_csv('data/best_features_rfc.csv', index=False)\n",
    "\n",
    "X = pd.read_csv('data/best_features_rfc.csv')\n",
    "y_test, pred = train(RandomForestClassifier(), X=X, y=y)\n",
    "print_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression and the SVM with a linear kernel both performed better than the baseline. We will try to improve both models using their hyperparameters. From https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using grid search to find best parameters\n",
    "\n",
    "# X = pd.read_csv('data/best_features_rfc.csv')\n",
    "# y = pd.read_csv('data/games.csv')['wl_home']\n",
    "\n",
    "# def find_best_params(X, y, model, params):\n",
    "#     grid = GridSearchCV(model, params, cv=TimeSeriesSplit(), scoring=accuracy_score ,n_jobs=-1, verbose=2)\n",
    "#     grid.fit(X, y)\n",
    "#     return grid.best_params_\n",
    "\n",
    "# params = {\n",
    "#     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     'max_depth': [None, 5, 10, 15, 20],\n",
    "#     'min_samples_split': [2, 5, 10, 15, 20],\n",
    "#     'min_samples_leaf': [1, 2, 5, 10, 15, 20]\n",
    "# }\n",
    "\n",
    "# best_params = find_best_params(X, y, RandomForestClassifier(), params)\n",
    "\n",
    "# pd.DataFrame(best_params, index=[0]).to_csv('data/best_params_rfc.csv', index=False)\n",
    "best_params = pd.read_csv('data/best_params_rfc.csv').to_dict('records')[0]\n",
    "\n",
    "print(best_params)\n",
    "print(len(best_features.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6670\n",
      "Precision: 0.6891\n",
      "Recall: 0.7497\n",
      "F1 Score: 0.7181\n"
     ]
    }
   ],
   "source": [
    "y_test, pred = train(RandomForestClassifier(**best_params), X=X, y=y)\n",
    "print_metrics(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
